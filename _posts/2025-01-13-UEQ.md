---
layout: post
title: User Experience Questionnaire
category: "user experience"
pubyear: 2008
pubauthor: Laugwitz et al.
scalename: UEQ
---

**This is the most up to date version of this scale.**

# Construct Summary

The authors define user experience as:

>"feelings, impressions, and attitudes that arise when experiencing the product under investigation" (p. 64)
 

# Rating = 85% 

<table>
  <thead>
    <tr>
      <th>Check?</th>
      <th>Guideline Item</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&#10003;</td>
      <td>Is the construct defined?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Does the final version of the items capture the construct as it has been defined by the authors?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Is the item generation process discussed (e.g., literature review, Delphi method, crowd-sourcing)?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Person to items 10:1 for the initial set of items?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they perform an EFA, PCA, Rasch, or similar test to determine the item to factor relationship?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they describe how they determined number of factors?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Did they report the full initial set of items?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they provide loadings (EFA) or item fits (Rasch) of all items?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Is there a description of the item removal process (e.g., using infit/outfit, factor loading minimum value, or cross-loading values)?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they list the final items included in the scale?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they include a factor structure test (e.g., second EFA, CFA, DIF, test for unidimensionality when using Rasch, or similar)?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Was a measure of reliability (e.g., Cronbach’s alpha, McDonalds Omega_h or Omega_t, Tarkkonen’s Rho) reported?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Was a test of validity (e.g., predictive, concurrent, convergent, discriminant) reported?</td>
    </tr>
  </tbody>
</table>

**Comments**
Unclear which aspects of the development process reported in the paper pertain to the German or English version of the scale.

# Reviewed by Experts &#10003;


# Downloads
[PAPER](https://link.springer.com/chapter/10.1007/978-3-540-89350-9_6){:target="_blank"}
<br>Laugwitz, B., Held, T., & Schrepp, M. (2008). Construction and evaluation of a user experience questionnaire. In HCI and Usability for Education and Work: 4th Symposium of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Society, USAB 2008, Graz, Austria, November 20-21, 2008. Proceedings 4 (pp. 63-76). Springer Berlin Heidelberg.

<br>PDF of scale as well as instructions for administration and scoring are not readily available. Check the paper for more details or email hriscaledatabase@gmail.com submit this information if you are the author of this scale.

# Final Scale Items (26 total):

**Attractiveness**
<br>annoying-enjoyable
<br>unlikable-pleasing
<br>good-bad
<br>attractive-unattractive
<br>friendly-unfriendly
<br>unpleasant-pleasant

**Perspicuity**
<br>not understandable-understandable
<br>easy to learn-difficult to learn
<br>complicated-easy
<br>clear-confusing

**Stimulation**
<br>valuable-inferior
<br>boring-exiting
<br>not interesting-interesting
<br>motivating-demotivating

**Efficiency**
<br>fast-slow
<br>inefficient-efficient
<br>impractical-practical
<br>organized-cluttered

**Novelty**
<br>inventive-conventional
<br>creative-dull
<br>usual-leading edge
<br>conservative-innovative 

**Dependability**
<br>obstructive-supportive
<br>unpredictable-predictable
<br>secure-not secure
<br>meets expectations-does not meet expectations




