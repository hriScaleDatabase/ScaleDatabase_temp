---
layout: post
title: Robot Threat Assessment (RoTA) Scale
category: "mental models"
pubyear: 2020
pubauthor: Matthews et al.
scalename: RoTA
---

**This is the most up to date version of this scale.**

# Construct Summary

>"The RoTA scale was developed to measure the individual’s mental model of autonomous robot partners using physics-based and psychological judgments as elements." (p. 239)


# Rating = 38% 

<table>
  <thead>
    <tr>
      <th>Check?</th>
      <th>Guideline Item</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&#10003;</td>
      <td>Is the construct defined?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Does the final version of the items capture the construct as it has been defined by the authors?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Is the item generation process discussed (e.g., literature review, Delphi method, crowd-sourcing)?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Person to items 10:1 for the initial set of items?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they perform an EFA, PCA, Rasch, or similar test to determine the item to factor relationship?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Did they describe how they determined number of factors?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Did they report the full initial set of items?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Did they provide loadings (EFA) or item fits (Rasch) of all items?</td>
    </tr>
    <tr>
      <td>NA</td>
      <td>Is there a description of the item removal process (e.g., using infit/outfit, factor loading minimum value, or cross-loading values)?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Did they list the final items included in the scale?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Did they include a factor structure test (e.g., second EFA, CFA, DIF, test for unidimensionality when using Rasch, or similar)?</td>
    </tr>
    <tr>
      <td>&#10003;</td>
      <td>Was a measure of reliability (e.g., Cronbach’s alpha, McDonalds Omega_h or Omega_t, Tarkkonen’s Rho) reported?</td>
    </tr>
    <tr>
      <td style="color: red;">&#10006;</td>
      <td>Was a test of validity (e.g., predictive, concurrent, convergent, discriminant) reported?</td>
    </tr>
  </tbody>
</table>

**Comments**
Items counted here are scenarios as that is what the EFA was performed on. The more classic way to interpret items would be the three individual questions reported on page 239 in the paper. The EFA was conducted only on responses to Q2. CFA was conducted on the same sample as the EFA which is not considered best practice and so a point was not awarded for that guideline item. The scale also did not get credit for validity since there was no second sample to correlate scales with other measures.

# Reviewed by Experts &#10003;


# Downloads
[PAPER](https://ieeexplore.ieee.org/abstract/document/8908731?casa_token=7sgCI-KuMWEAAAAA:52LHnsvO0yzxargRQjgbGy62I9WO_BK03vHepH0sbP0eA3ZIedIBKRyNptccF7JnhKLEmf-NBQ){:target="_blank"}
<br>Matthews, G., Lin, J., Panganiban, A. R., & Long, M. D. (2019). Individual differences in trust in autonomous robots: Implications for transparency. IEEE Transactions on Human-Machine Systems, 50(3), 234-244.

<br>PDF of instructions for administration and score not readily available. Check the paper for more details or email hriscaledatabase@gmail.com to submit this information if you are the author of this scale.

# Final Scale Items (20 total):

Scenarios are not reported verbatim in text.